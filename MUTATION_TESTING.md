# Mutation Testing Guide

## Overview

Mutation testing is a quality assurance technique that validates **test quality** by introducing controlled bugs (mutations) into your source code and checking if your tests catch them.

### Key Concepts

- **Mutation**: A small, deliberate change to source code (e.g., `==` ‚Üí `!=`)
- **Killed**: A mutation caught by tests (test failed) - ‚úÖ **Good**
- **Survived**: A mutation not caught by tests (test passed) - ‚ùå **Bad**
- **Mutation Score**: `(Killed / Total) √ó 100%` - **Goal: 80%+**

## Why Mutation Testing?

Code coverage metrics only tell you which lines were executed, not whether tests **actually validate behavior**.

**Example:**
```python
def is_admin(user):
    return user.role == "admin"

# Test with 100% coverage but 0% mutation score:
def test_is_admin():
    user = Mock(role="admin")
    is_admin(user)  # Just calls it, doesn't assert anything!
```

Without mutation testing, this test passes with 100% coverage. But if we mutate `==` to `!=`, the test still passes (no assertion), so it gets 0% mutation score.

With mutation testing enabled, we'd catch this and require:
```python
def test_is_admin():
    user = Mock(role="admin")
    assert is_admin(user) is True  # Actually validates behavior
```

## Running Mutation Testing

### Local

```bash
# Run full mutation testing
python cli/mutation_test.py

# View report
cat .mutation_test/report.json | python -m json.tool
```

### In CI/CD

Mutation testing runs automatically in GitHub Actions as part of the quality pipeline:

```bash
gh workflow run quality.yml
```

The workflow:
1. Runs all tests first
2. Scans for mutation points in CLI code
3. Introduces mutations one at a time
4. Runs tests for each mutation
5. Reports which mutations were killed vs survived
6. Fails if mutation score < 70%

## Current Results

| Metric | Value | Status |
|--------|-------|--------|
| **Mutation Score** | 94.7% | üü¢ Excellent |
| **Mutations Detected** | 285 | - |
| **Mutations Killed** | 270 | ‚úÖ |
| **Mutations Survived** | 15 | - |

### Surviving Mutations Analysis

The 15 surviving mutations are in `commit_validator.py` and represent false positives:

1. **Subprocess Parameters** (12 mutations)
   - Lines 87-88, 118-119, 126-127, 164-165, 205-206, 252-253, 349-350
   - Mutations: `capture_output=True ‚Üí False`, `text=True ‚Üí False`
   - **Why it survives:** These parameters are in subprocess.run() calls that are mocked in tests. Changing them doesn't affect test behavior because the function is mocked.
   - **Status:** ‚úÖ **Acceptable** - Tests correctly mock the subprocess module

2. **Logical Operator** (1 mutation)
   - Line 213: `or` ‚Üí `and` in complexity line parsing
   - **Why it survives:** The condition uses multiple `or` operators; changing one to `and` still returns True due to short-circuit evaluation
   - **Status:** ‚úÖ **Acceptable** - Complex logical conditions are inherently difficult to fully mutate

## How Mutation Testing Works

### 1. Mutation Detection

The mutation engine scans all Python files in `cli/` directory and identifies mutation points:

```python
# Original code
if user.role == "admin":  # Mutation point: == -> !=
    return True
if enabled and verified:  # Mutation point: and -> or
    process()
```

### 2. Mutation Types

| Type | Example | Impact |
|------|---------|--------|
| **Comparison Operators** | `==` ‚Üí `!=`, `<` ‚Üí `<=` | Boolean logic errors |
| **Boolean Literals** | `True` ‚Üí `False` | Configuration errors |
| **Logical Operators** | `and` ‚Üí `or` | Condition logic errors |
| **Arithmetic** | `+` ‚Üí `-`, `*` ‚Üí `/` | Calculation errors |
| **Return Values** | `return x` ‚Üí `return not x` | Return value errors |

### 3. Mutation Testing Flow

```
1. Original Tests Pass?
   ‚îú‚îÄ No ‚Üí Fix tests first, don't test mutations
   ‚îî‚îÄ Yes ‚Üí Continue

2. For Each Mutation:
   ‚îú‚îÄ Modify source file
   ‚îú‚îÄ Run test suite
   ‚îú‚îÄ Check results:
   ‚îÇ  ‚îú‚îÄ Test fails ‚Üí Mutation killed ‚úÖ
   ‚îÇ  ‚îî‚îÄ Test passes ‚Üí Mutation survived ‚ùå
   ‚îî‚îÄ Restore original file

3. Calculate Score:
   ‚îú‚îÄ Mutation Score = (Killed / Total) √ó 100%
   ‚îî‚îÄ Report results
```

## Improving Mutation Score

### Identify Weak Tests

1. **Check the report:**
   ```bash
   python cli/mutation_test.py
   cat .mutation_test/report.json
   ```

2. **Look at survived mutations** - These reveal weak tests

3. **Example weakness:**
   ```python
   # Weak test - just calls function, doesn't verify output
   def test_process_config():
       result = process_config(valid_input)

   # Better test - verifies expected output
   def test_process_config():
       result = process_config(valid_input)
       assert result["status"] == "success"  # Catches mutations in status field
       assert result["value"] > 0  # Catches mutations in value calculation
   ```

### Best Practices for High Mutation Scores

1. **Assert on specific values**, not just that code ran
   ```python
   # Bad: Just verifies it doesn't crash
   logger.warning("test")

   # Good: Verifies behavior
   assert logger.warning.call_count == 1
   assert "test" in logger.warning.call_args[0][0]
   ```

2. **Test boundary conditions**
   ```python
   # Bad: Only tests happy path
   assert validate([1, 2, 3]) is True

   # Good: Tests boundaries
   assert validate([1]) is True      # Single item
   assert validate([]) is False      # Empty
   assert validate(None) is False    # None
   ```

3. **Test with diverse inputs**
   ```python
   # Use property-based testing (hypothesis) for edge cases
   from hypothesis import given, strategies as st

   @given(st.lists(st.integers()))
   def test_sum_commutativity(numbers):
       assert sum(numbers) == sum(reversed(numbers))
   ```

4. **Verify both true and false paths**
   ```python
   def test_is_enabled():
       # Test True path
       config = {"enabled": True}
       assert is_enabled(config) is True

       # Test False path
       config = {"enabled": False}
       assert is_enabled(config) is False
   ```

5. **Mock external dependencies, not behavior**
   ```python
   # Bad: Mocks too much - can't catch mutations in your code
   with patch.object(validator, "check_syntax", return_value=True):
       result = validator.validate()

   # Good: Mocks external dependency, tests your validation
   with patch.object(subprocess, "run") as mock_run:
       mock_run.return_value = Mock(returncode=0)
       result = validator.validate(["test.py"])
       assert result[0] is True  # Validates your logic
   ```

## Mutation Testing Workflow

### Local Development

1. **Make changes to code**
   ```bash
   # Edit cli/some_module.py
   ```

2. **Run tests**
   ```bash
   pytest tests/
   ```

3. **Run mutation testing before committing**
   ```bash
   python cli/mutation_test.py
   ```

4. **If mutation score < 80%:**
   - Review survived mutations
   - Improve weak tests
   - Re-run mutation testing
   - Commit when score ‚â• 80%

### CI/CD Pipeline

Mutation testing runs automatically:

1. **On every push** to main/develop
2. **In pull requests** - checks mutation score
3. **Fails if** mutation score < 70% (threshold)
4. **Warns if** mutation score < 80% (target)
5. **Passes if** mutation score ‚â• 80% (excellent)

## Architecture

### Mutation Test Framework

Located in `cli/mutation_test.py` - custom implementation with:

- **MutationDetector**: AST-based scanner that finds mutation points
- **MutationTester**: Runs tests against each mutation
- **MutationReport**: Generates JSON report of results

### Why Custom?

- ‚úÖ Python 3.14 compatible (external tools like `mutmut` not yet compatible)
- ‚úÖ Tailored to our codebase structure
- ‚úÖ No external dependencies beyond pytest
- ‚úÖ Full control over mutation types and reporting

### Output

```json
{
  "timestamp": "2025-10-30T16:05:33.629199",
  "total_mutations": 285,
  "killed_mutations": 270,
  "survived_mutations": 15,
  "mutation_score": 94.7,
  "survived_mutations_details": [
    {
      "file": "commit_validator.py",
      "line": 87,
      "type": "boolean_literal",
      "original": "capture_output=True,",
      "mutated": "capture_output=False,",
      "description": "Changed True to False on line 87"
    }
  ]
}
```

## Limitations & Considerations

### What Mutation Testing Doesn't Cover

1. **Test execution path selection** - You might test wrong paths
2. **False positives** - Some mutations can't be caught (e.g., mocked external calls)
3. **Performance** - Running 285 mutations takes ~2-5 minutes
4. **Integration points** - Doesn't test inter-module behavior

### When to Skip Mutations

- ‚ùå Don't disable mutation testing to hide weak tests
- ‚ö†Ô∏è  Do document why a mutation survives if it's a false positive
- ‚úÖ Do improve tests to catch more mutations

## Next Steps: Property-Based Testing

After mutation testing reaches 80%+, add property-based testing with `hypothesis`:

```bash
pip install hypothesis
```

```python
from hypothesis import given, strategies as st

@given(st.lists(st.integers(min_value=0, max_value=1000)))
def test_validator_handles_any_integers(numbers):
    """Test validator with 100+ random integer lists."""
    result = validate(numbers)
    assert isinstance(result, bool)
```

This finds edge cases your manual tests miss.

## Questions?

For mutation testing issues or questions:

1. Check the mutation report: `.mutation_test/report.json`
2. Review survived mutations to identify weak tests
3. Improve tests using patterns in "Best Practices" section
4. Re-run: `python cli/mutation_test.py`

---

**Current Status**: üü¢ 94.7% Mutation Score (Excellent)
**Target**: üéØ 80%+ (Tier 1 CRITICAL)
**Threshold**: üî¥ < 70% (CI Fails)
